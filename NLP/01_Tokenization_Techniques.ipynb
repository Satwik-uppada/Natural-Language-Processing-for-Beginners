{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27198302-ab7c-40a8-9d1e-8c0b68496e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e37702-c87c-45b9-a207-379fdc3493f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59bc802-fcb7-468b-bdd2-8e91d3058526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the nltk data for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# refers to a pre-trained unsupervised machine learning model for sentence tokenization\n",
    "# downloads the Punkt tokenizer models and other necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3790ad8-5a4d-409d-a3bd-2a2d78c5cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a given text\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9a17cc-74b8-4d0e-a05b-f388e783f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = \"Tokenization is the process of breaking a text into individual units, such as words or phrases.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf58347-b0f2-422c-9ef8-ab46eeb1b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sample text\n",
    "tokens = tokenize_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da6461e-553a-46a7-802f-cda8d451c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'a', 'text', 'into', 'individual', 'units', ',', 'such', 'as', 'words', 'or', 'phrases', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb06cb5",
   "metadata": {},
   "source": [
    "## Using a Text File as Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60cdfc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8470e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for tokenization\n",
    "def tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file: #uni-code transformation format\n",
    "        text = file.read()\n",
    "        tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76cc98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_file.txt' with the path to your text file\n",
    "file_path = 'demo.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "229f171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the content of the file\n",
    "tokens = tokenize_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a27f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Tokenization', 'involves', 'breaking', 'a', 'text', 'document', 'into', 'pieces', 'that', 'a', 'machine', 'can', 'understand', ',', 'such', 'as', 'words', '.', 'Now', ',', 'you', '’', 're', 'probably', 'pretty', 'good', 'at', 'figuring', 'out', 'what', '’', 's', 'a', 'word', 'and', 'what', '’', 's', 'gibberish', '.', 'English', 'is', 'especially', 'easy', '.', 'See', 'all', 'this', 'white', 'space', 'between', 'the', 'letters', 'and', 'paragraphs', '?', 'That', 'makes', 'it', 'really', 'easy', 'to', 'tokenize', '.', 'So', ',', 'NLP', 'rules', 'are', 'sufficient', 'for', 'English', 'tokenization', '.', 'But', 'how', 'do', 'you', 'teach', 'a', 'machine', 'learning', 'algorithm', 'what', 'a', 'word', 'looks', 'like', '?', 'And', 'what', 'if', 'you', '’', 're', 'not', 'working', 'with', 'English-language', 'documents', '?', 'Logographic', 'languages', 'like', 'Mandarin', 'Chinese', 'have', 'no', 'whitespace', '.', 'This', 'is', 'where', 'we', 'use', 'machine', 'learning', 'for', 'tokenization', '.', 'Chinese', 'follows', 'rules', 'and', 'patterns', 'just', 'like', 'English', ',', 'and', 'we', 'can', 'train', 'a', 'machine', 'learning', 'model', 'to', 'identify', 'and', 'understand', 'them', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45697c3",
   "metadata": {},
   "source": [
    "## Various Tokenization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77ab28",
   "metadata": {},
   "source": [
    "### 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7a4abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Word', 'tokenization', 'is', 'important', 'for', 'natural', 'language', 'processing', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Word tokenization is important for natural language processing.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8eb828",
   "metadata": {},
   "source": [
    "### 2. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "071f6340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Sentence tokenization breaks text into sentences.', 'This is an example.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Sentence tokenization breaks text into sentences. This is an example.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097fdcc5",
   "metadata": {},
   "source": [
    "### 3. Whitespace Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218c0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokens: ['Whitespace', ' tokenization', ' splits text based on spaces.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Whitespace, tokenization, splits text based on spaces.\"\n",
    "tokens = text.split(',')\n",
    "print(\"Whitespace Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4256a",
   "metadata": {},
   "source": [
    "### 4. Custom Delimiter Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4091854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Delimiter Tokens: ['Custom', 'delimiter', 'tokenization', '', 'separating', 'items:', 'apple', '', 'orange', '', 'banana.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Custom delimiter tokenization, separating items: apple, orange, banana.\"\n",
    "tokens = re.split(r'[,\\s]', text)\n",
    "print(\"Custom Delimiter Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066476d4-4d3b-4451-ab57-4bf1f7c80202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
