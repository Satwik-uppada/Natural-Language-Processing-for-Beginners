{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1><b> NLP ( Natural Language Processing ) </b></h1> </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[`Table of contents:`](#toc0_)\n",
    "- Definition\n",
    "- NLTK vs SpaCy\n",
    "- Basic Terms\n",
    "- Understanding of Text Analysis\n",
    "- Tokenization\n",
    "  - Types of Tokenization\n",
    "    - Word Tokenizer\n",
    "    - Sentence Tokenizer\n",
    "    - Wordpunct Tokenizer\n",
    "    - TreebankWord Tokenizer\n",
    "    - Tweet Tokenizer\n",
    "    - Custom Tokenizer\n",
    "      - Whitespace Tokenizer\n",
    "      - Delimeter Tokenizer using regular expressions\n",
    "- Normalization\n",
    "  - Techniques of Normalization\n",
    "    - Stemming\n",
    "      - Types of Stemming\n",
    "        - PorterStemmer\n",
    "        - SnowballStemmer\n",
    "        - RegexpStemmer\n",
    "        - LancasterStemmer\n",
    "    - Lemmatization\n",
    "- Stopwords\n",
    "- POS Tags\n",
    "  - pos_tag\n",
    "  - pos_tag_sent\n",
    "- Names Entity Recognition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DEFINITION`:\n",
    "NLTK (Natural Language Toolkit) is a Python library for working with human language data, offering tools for tasks like tokenization, stemming, tagging, parsing, and more. Its current version, as of my last update, is NLTK 3.8.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[` Differences between NLTK vs SpaCy`](#toc0_)\n",
    "\n",
    "| Feature               | NLTK                                             | SpaCy                                            |\n",
    "|-----------------------|--------------------------------------------------|--------------------------------------------------|\n",
    "| Language Support      | Supports multiple languages.                     | Supports multiple languages.                     |\n",
    "| Performance           | Slower processing speed.                          | Faster processing speed.                         |\n",
    "| Ease of Use           | Simple interface, but may require more coding.   | More complex interface, but easier to use.       |\n",
    "| Tokenization          | Basic tokenization functionalities.              | Advanced tokenization with context awareness.    |\n",
    "| POS Tagging           | Basic Part-of-Speech tagging capabilities.       | Advanced POS tagging with deep learning models. |\n",
    "| Dependency Parsing    | Limited dependency parsing capabilities.         | Accurate and efficient dependency parsing.      |\n",
    "| Named Entity Recognition (NER) | Basic NER capabilities.                   | Advanced NER with pre-trained models.           |\n",
    "| Entity Linking        | Not supported.                                   | Supported for resolving entities to knowledge bases. |\n",
    "| Active Development    | Less active development community.               | Active development with regular updates.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[`Basics terms:`](#toc0_)\n",
    "\n",
    "---\n",
    "\n",
    "`Corpus:` collection of text documents(tweets,comments)<br>\n",
    "\n",
    "`Tokens:` smaller units of a text(words, phrases, ngrams)<br>\n",
    "\n",
    "`Ngrams:` combination of N words / characters together.<br>\n",
    "\n",
    "Ex: Sentence: I LOVE MY PHONE\n",
    "  \n",
    "- Unigrams(n=1) : I, Love, My, Phone.<br>\n",
    "- Bigrams(n=2): I Love, Love My, My Phone.<br>\n",
    "- trigrams(n=3): I Love my, Love My Phone.<br>\n",
    "\n",
    "---\n",
    "\n",
    "Corpus > Documents > Paragraphs > Sentences > words\n",
    "\n",
    "---\n",
    "simply, we can remember as\n",
    "\n",
    "1. Corpus -> paragraphs\n",
    "\n",
    "2. Documents -> sentences\n",
    "\n",
    "3. vocabulary -> unique words in the sentences\n",
    "\n",
    "---\n",
    "`Encoding standard:`\n",
    "- Each character has a unique representation code on computers\n",
    "- ASCII (American Standard Code for Information Interchange) was widely popular standard that mostly encodes all English characters\n",
    "- The Unicode standard helps represnt text in almost all the languages of the world\n",
    "  - `UTF-16`: Encodes each character with a 16-bit code\n",
    "  - `UTF-8`: Encodes each English character with an 8-bit code and each non-english character with a 32-bit code.   \n",
    "- `UTF-8`:\n",
    "  - English character store in 8 bit size.  --> advantage of UTF-8.\n",
    "  - But, when it comes to symbols. it takes 24 bit size --> disadvantage of UTF-8\n",
    "- `UTF-16`:\n",
    "  - English character store in 16 bit size --> waste of memory --> disadvantage of UTF-16.\n",
    "  - But, when it comes to symbols. it takes 16 bits which is much lesser than UTF-8 --> advantage of UTF-16.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## <a id='toc1_3_'></a>[`Corpus` -> Paragraphs](#toc0_)\n",
    "\n",
    "---\n",
    "\n",
    "\"My name is satwik, i have interest in learning ML, NLP, and DL. I am also a data enthusiastic\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is satwik, i have interest in learning ML, NLP, and DL.\\nI am also a \"data enthusiastic\".\\nWelcome to sathvi\\'s world.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\"\"My name is satwik, i have interest in learning ML, NLP, and DL.\n",
    "I am also a \"data enthusiastic\".\n",
    "Welcome to sathvi's world.\"\"\"\n",
    "\n",
    "corpus # printing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[`Understanding of Text Analysis`](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <a id='toc2_1_1_'></a>[**`Process of NLP:`**](#toc0_)\n",
    "Lexical analysis -> syntactic analysis -> semantic analysis -> pragmatic analysis -> Disclosure integration\n",
    "\n",
    "\n",
    "**`Lexcial analysis:`**\n",
    "-\tFirst step in NLP\n",
    "-\tBreaking down input text into smaller units called tokens(words,puntuations,etc..)\n",
    "-\tRemove irrelavent characters or noise from text -> Removal of stopwords, irrelavant symbols.\n",
    "-   we use preprocessing steps like Word Frequencies and Stop Words, Tokenisation, Bag-of-Words\n",
    "Representation, Stemming and Lemmatization, Final Bag-of-Words Representation, TF-IDF\n",
    "Representation, Canonicalisation, Phonetic Hashing, Edit Distance, Pointwise Mutual Information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**`Syntactic analysis:`**\n",
    "-\tAlso known as parsing --> POS Tagging, NER, parsing \n",
    "-\tAnalyze grammatical structure, to understand how tokens are arranged, how they relate to each other.\n",
    "\n",
    "**Ex:** sub -verb – obj relationship\n",
    "\n",
    "\n",
    "**`Semantic analysis:`**\n",
    "-\tAnalyze the meaning of the text, to understand what the text is trying to convey.\n",
    "-   we use processing steps like --> cosine similarity, wordsense disambigustion, wordnet, knowledge graphs\n",
    "\n",
    "**`Pragmatic analysis:`**\n",
    "-\tUnderstanding the implied meaning of text beyond its literal interpretation. \n",
    "-\tFactors like context, speaker intention, inference, social cues.\n",
    "\n",
    "**Ex:** The dishes aren’t going to wash themselves\n",
    "\n",
    "\n",
    "\n",
    "**`Discourse integration:`**\n",
    "-\tCombining the individual interpretations of sentences or phrases to understand entire text.\n",
    "-\tConnects the related piecees of information, and organizing the the extracted information into structured representation.\n",
    "-\tIt ensures that overall meaning of the text is properly understood cand be used for further analysis\n",
    "\n",
    "**Ex:**\n",
    "-\t**Mom:** \"The dishes aren't going to wash themselves.\"\n",
    "-\t**You:** \"I'll do them after I finish my homework.\"\n",
    "-\t**Your sister:** \"I can help you with the dishes.\"\n",
    "-\t**Dad:** \"That sounds like a good plan. I'll take care of vacuuming the living room while you two do the dishes.\"\n",
    "-\tNow, By taking entire text into account,we can figure out the complete and accurate meaning of it.\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[`Text Preprocessing`](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='toc3_1_'></a>[`Tokenization`](#toc0_)\n",
    "---\n",
    "- Tokenization refers to break down the text into smaller units(tokens).<br>\n",
    "Smaller units can be words, numbers, symbols,ngrams, characters, paragraphs.\n",
    "   - splits paragraphs into sentences and sentences into words.\n",
    "\n",
    "- There are two major kinds of tokenization techniques:\n",
    "  - Word Tokenization\n",
    "  - Sentence Tokenization\n",
    "  - Tweet Tokenization\n",
    "  - Tokenization using regex(character,custom)\n",
    "  - Whitespace tokenization\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing sentence tokenization.\n",
    "\n",
    "`Tokens` -> (Documents/sentences)\n",
    "---\n",
    "1. My name is satwik, i have an interest in learning ML, NLP and DL.\n",
    "2. I am also a data enthusiastic.\n",
    "\n",
    "---\n",
    "If we perform further tokenization(word tokenization) for these sentences.\n",
    "\n",
    "`Tokens` -> (words)\n",
    "---\n",
    "\n",
    "1. My\n",
    "2. name\n",
    "3. is\n",
    "4. .....\n",
    ".\n",
    ".\n",
    ".\n",
    "n. enthusiastic.\n",
    "---\n",
    "\n",
    "Let's go and understand what is vocabulary with another example:\n",
    "`\"I like to drink Apple juice. My friend likes mango juice\".`\n",
    "\n",
    "if we want to find unique words in it.\n",
    "\n",
    "1. I like to drink apple juice.\n",
    "2. My friend likes mango juice.\n",
    "\n",
    "if we count the number of words in these sentences, we get 11 words as total.\n",
    "\n",
    "But if we count the number of unique words in these sentences we only get 10 as total. Because juice is repeating right... so, vocab tokenzation remove duplicate words and send the unique words in the sentence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[`Sentence Tokenization`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is satwik, i have interest in learning ML, NLP, and DL.',\n",
       " 'I am also a \"data enthusiastic\".',\n",
       " \"Welcome to sathvi's world.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Tokenization\n",
    "# Paragraphs --> sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "doc= sent_tokenize(corpus)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. My name is satwik, i have interest in learning ML, NLP, and DL.\n",
      "2. I am also a \"data enthusiastic\".\n",
      "3. Welcome to sathvi's world.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i=1\n",
    "for sentence in doc:\n",
    "    print(\"{}. {}\".format(i,sentence))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[`Word Tokenization`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'satwik', ',', 'i', 'have', 'interest', 'in', 'learning', 'ML', ',', 'NLP', ',', 'and', 'DL', '.', 'I', 'am', 'also', 'a', '``', 'data', 'enthusiastic', \"''\", '.', 'Welcome', 'to', 'sathvi', \"'s\", 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenization\n",
    "# Paragraph --> words \n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['My', 'name', 'is', 'satwik', ',', 'i', 'have', 'interest', 'in', 'learning', 'ML', ',', 'NLP', ',', 'and', 'DL', '.'], ['I', 'am', 'also', 'a', '``', 'data', 'enthusiastic', \"''\", '.'], ['Welcome', 'to', 'sathvi', \"'s\", 'world', '.']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sentence --> words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = [word_tokenize(sentence) for sentence in doc]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_3_'></a>[`WordPunct Tokenization`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'satwik', ',', 'i', 'have', 'interest', 'in', 'learning', 'ML', ',', 'NLP', ',', 'and', 'DL', '.', 'I', 'am', 'also', 'a', '\"', 'data', 'enthusiastic', '\".', 'Welcome', 'to', 'sathvi', \"'\", 's', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sentence --> words with punctuation separated\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(corpus))\n",
    "\n",
    "# this function make punctuations as a separate string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that in this output, we can find '(apostrophe) as a string. \n",
    "But in word_tokenizer (') is not separated with after letter.\n",
    "this is the difference of word_tokenize vs wordpunct_tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_4_'></a>[`TreebankWord Tokenization`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'satwik', ',', 'i', 'have', 'interest', 'in', 'learning', 'ML', ',', 'NLP', ',', 'and', 'DL.', 'I', 'am', 'also', 'a', '``', 'data', 'enthusiastic', \"''\", '.', 'Welcome', 'to', 'sathvi', \"'s\", 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find '.' is included in previous word. If you observe in wordpunct_tokenizer, '.' is printed as separate string.\n",
    "- But at the endding word (.) is not included to word. [special case of TreebankWord Tokenization]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Tokenizer            | Unique Difference                                                                    | Example                                                      |\n",
    "|----------------------|--------------------------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| word_tokenize        | Retains punctuation(',\",!..) as separate tokens.                                              | `'enthusiastic', '.'`, 'Welcome', 'to', 'sathvi', `\"'s\"`, 'world', '.'       |\n",
    "| wordpunct_tokenize   | Splits tokens based on whitespace and punctuation marks, treats period '.' as separate token. |  `'enthusiastic', '.'`, 'Welcome', 'to', 'sathvi', `\"'\", 's'`, 'world', '.'   |\n",
    "| TreebankWordTokenizer| Splits tokens based on Penn Treebank conventions, treats period '.' as part of the previous word.and it won't include ending '.' with previous word. | `'enthusiastic.'`, 'Welcome', 'to', 'sathvi', `\"'s\", 'world', '.'`   | \n",
    "\n",
    "\n",
    "- Observe the highlighted parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_5_'></a>[`Tweet Tokenization`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Loving',\n",
       " 'the',\n",
       " 'new',\n",
       " 'features',\n",
       " 'of',\n",
       " 'the',\n",
       " '#OpenAI',\n",
       " 'API',\n",
       " '!',\n",
       " 'Check',\n",
       " 'it',\n",
       " 'out',\n",
       " 'at',\n",
       " 'https://openai.com',\n",
       " '@OpenAI',\n",
       " '😊']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet =  \"Loving the new features of the #OpenAI API! Check it out at https://openai.com @OpenAI 😊\"\n",
    "tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_6_'></a>[`Custom Tokenization`](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***WhiteSpace Tokenization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokens: ['Whitespace', 'tokenization', 'splits', 'text', 'based', 'on', 'spaces.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Whitespace tokenization splits text based on spaces.\"\n",
    "tokens = text.split()\n",
    "print(\"Whitespace Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Delimiter Tokenization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Delimiter Tokens: ['Custom', 'delimiter', 'tokenization', '', 'separating', 'items', 'based', 'on', 'delimiters:', 'apple', '', 'orange', '', 'banana.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Custom delimiter tokenization, separating items based on delimiters: apple, orange, banana.\"\n",
    "tokens = re.split(r'[,\\s]', text)\n",
    "print(\"Custom Delimiter Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_7_'></a>[`Tokenize Text file data`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'you', 'ever', 'wanted', 'to', 'build', 'a', 'web', 'app', 'for', 'your', 'data', 'science', 'project', '?', 'But', 'perhaps', 'did', 'not', 'because', 'of', 'the', 'extensive', 'time', 'needed', 'to', 'code', 'the', 'web', 'app', 'or', 'intimidated', 'by', 'Django', 'and', 'flask', '?', 'In', 'this', 'video', ',', 'I', 'will', 'show', 'you', 'how', 'to', 'build', 'a', 'machine', 'learning', 'powered', 'data', 'science', 'web', 'app', 'in', 'Python', 'using', 'the', 'streamlit', 'library', 'in', 'less', 'than', '50', 'lines', 'of', 'code', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define a function for tokenization\n",
    "import nltk\n",
    "def tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file: #uni-code transformation format\n",
    "        text = file.read()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "print(tokenize_file('demo.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[`Conclusion`](#toc0_)\n",
    "---\n",
    "| Tokenization Technique     | Importing Statement                        | Method                   | Example                                                   |\n",
    "|----------------------------|--------------------------------------------|--------------------------|-----------------------------------------------------------|\n",
    "| word_tokenize              | `from nltk.tokenize import word_tokenize`  | `word_tokenize()`        | `text = \"This is an example sentence.\"`<br>`tokens = word_tokenize(text)` |\n",
    "| wordpunct_tokenize         | `from nltk.tokenize import wordpunct_tokenize` | `wordpunct_tokenize()`   | `text = \"This is an example sentence.\"`<br>`tokens = wordpunct_tokenize(text)` |\n",
    "| TreebankWordTokenizer     | `from nltk.tokenize import TreebankWordTokenizer` | `TreebankWordTokenizer().tokenize()` | `text = \"This is an example sentence.\"`<br>`tokens = TreebankWordTokenizer().tokenize(text)` |\n",
    "| sent_tokenize             | `from nltk.tokenize import sent_tokenize`  | `sent_tokenize()`        | `text = \"This is an example sentence. Another sentence.\"`<br>`sentences = sent_tokenize(text)` |\n",
    "| RegexpTokenizer           | `from nltk.tokenize import RegexpTokenizer` | `RegexpTokenizer().tokenize()` | `tokenizer = RegexpTokenizer(r'\\w+')`<br>`text = \"This is an example sentence.\"`<br>`tokens = tokenizer.tokenize(text)` |\n",
    "| WhitespaceTokenizer       | `from nltk.tokenize import WhitespaceTokenizer` | `WhitespaceTokenizer().tokenize()` | `text = \"This is an example sentence.\"`<br>`tokens = WhitespaceTokenizer().tokenize(text)` |\n",
    "| TweetTokenizer            | `from nltk.tokenize import TweetTokenizer`    |   `TweetTokenizer().tokenize()`   |  `tweet =  \"Loving the new features of the #OpenAI API! Check it out at https://openai.com @OpenAI 😊\"`<br> `tokenizer.tokenize(tweet)`  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[`Normalization`](#toc0_)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization: Process of converting a token into its base form(morpheme).<br>\n",
    "\n",
    "Morpheme: Base form of a word.<br>\n",
    "\n",
    "Sturcture of token: < prefix > < `morpheme` > < suffix > <br>\n",
    "\n",
    "Example: `Antinationalist` - Anti + national + ist<br>\n",
    "\n",
    "here, morpheme is national base form of this word.<br>\n",
    "\n",
    "Normalization is helpful in reducing data dimensionality, text cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two popular methods are used for normalization:\n",
    "- Stemming\n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_4_'></a>[`Stemming`](#toc0_)\n",
    "\n",
    "---\n",
    "`Definition`: Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as `stem/root`.\n",
    "\n",
    "Ex: laughing, laughed, laughs, laugh --> laugh\n",
    "\n",
    "<u>Disadvantages:</u>\n",
    "- May generate non-meaningful terms\n",
    "\n",
    "- so, Not good for normalization\n",
    "\n",
    "Example for abnormal stemming words:\n",
    " \n",
    "- Studies -> Studi\n",
    "- Winning -> Winn\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Types of Stemming`:\n",
    "- Porter Stemmer\n",
    "- Snowball Stemmer\n",
    "- Regex-Based Stemmer\n",
    "- Lancaster Stemmer(aggressive stemming)\n",
    "- Lovins Stemmer( \" \" \" )\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words to perform stemming\n",
    "words = ['eating','eats','eaten','writing','writes','programming','programmer','history','finally','finalized']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[`PorterStemmer`:](#toc0_)\n",
    "\n",
    "One of the most widely used stemming algorithms, Porter Stemmer, is based on a series of heuristic replacement rules.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eaten\n",
      "writing ---> write\n",
      "writes ---> write\n",
      "programming ---> program\n",
      "programmer ---> programm\n",
      "history ---> histori\n",
      "finally ---> final\n",
      "finalized ---> final\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(i+\" ---> \" +stemming.stem(i))\n",
    "\n",
    "# .stem is a function to get stemming words from words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can find that some stemming words don't have meaning like histori and programm.\n",
    "- It is the disadvantage of stemming.\n",
    "- Let's check for some other words...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'studi')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly'),stemming.stem('studies'),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- 'studi' has no meaning in english right?...<br>\n",
    "- This is the major disadvantage of stemming.<br>\n",
    "- You can try the same with 'congratulations','goes', 'negative' ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc3_6_'></a>[`Snowball Stemmer`:](#toc0_)\n",
    "An improvement over the Porter Stemmer, Snowball Stemmer (also known as Porter2) offers better language support and more accurate stemming.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' We have multiple options of languages in snowball stemmer. \\nso we need to specify the language as an arguement.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "''' We have multiple options of languages in snowball stemmer. \n",
    "so we need to specify the language as an arguement.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eaten\n",
      "writing ---> write\n",
      "writes ---> write\n",
      "programming ---> program\n",
      "programmer ---> programm\n",
      "history ---> histori\n",
      "finally ---> final\n",
      "finalized ---> final\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in words:\n",
    "    print(i+\" ---> \" +snowball_stemmer.stem(i))\n",
    "\n",
    "# .stem is a function to get stemming words from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem('fairly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_7_'></a>[`Multilingual Stemming using Snowball Stemmer`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed English Text: i am run in the beauti garden\n",
      "Stemmed French Text: je cour dan le beau jardin\n"
     ]
    }
   ],
   "source": [
    "def snowball_stemming(text,lang):\n",
    "    stemmer_obj = SnowballStemmer(lang)\n",
    "    tokens = text.split()\n",
    "\n",
    "    stemmed_tokens = [stemmer_obj.stem(token.lower()) for token in tokens]\n",
    "    result = ' '.join(stemmed_tokens)\n",
    "    return result\n",
    "\n",
    "\n",
    "English_text = \"I am running in the beautiful gardens\"\n",
    "French_text = \"Je cours dans les beaux jardins\"\n",
    "stemming_english = snowball_stemming(English_text,\"english\")\n",
    "stemming_french = snowball_stemming(French_text, \"french\")\n",
    "print(\"Stemmed English Text:\", stemming_english)\n",
    "print(\"Stemmed French Text:\", stemming_french)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc3_7_1_'></a>[`Let's see the differences of PorterStemmer and SnowballStemmer`](#toc0_)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_words = ['fairly', 'sportingly', 'negative', 'goes', 'congratulations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Miss_Words |PorterStemmer|SnowballStemmer     |\n",
      "fairly  --> fairli --> fair \n",
      "sportingly  --> sportingli --> sport \n",
      "negative  --> neg --> negat \n",
      "goes  --> goe --> goe \n",
      "congratulations  --> congratul --> congratul \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"|Miss_Words |PorterStemmer|SnowballStemmer     |\")\n",
    "\n",
    "for i in miss_words:\n",
    "    print(\"{}  --> {} --> {} \".format(i,stemming.stem(i),snowball_stemmer.stem(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Miss_Words        | PorterStemmer   | SnowballStemmer |\n",
    "|-------------------|-----------------|-----------------|\n",
    "| fairly            | fairli          | fair            |\n",
    "| sportingly        | sportingli      | sport           |\n",
    "| negative          | neg             | negat           |\n",
    "| goes              | goe             | goe             |\n",
    "| congratulations   | congratul       | congratul       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can conclude that, Snowball Stemmer is some what better in some cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_8_'></a>[`RegexpStemmer class`](#toc0_)\n",
    "\n",
    "This is a simple stemming algorithm that uses regular expressions to match and remove common suffix or prefix that matched the expression.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegexpStemmer has 2 parameters\\n1. regexp: The regex that shiuld be used to identify morphological affixes.\\n2. min: The minimum length of string to stem.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$',min=4)\n",
    "\n",
    "\n",
    "'''\n",
    "RegexpStemmer has 2 parameters\n",
    "1. regexp: The regex that shiuld be used to identify morphological affixes.\n",
    "2. min: The minimum length of string to stem.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(reg_stemmer.stem('eating'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to undestand what is happening here....\n",
    "\n",
    "Basically, we are writing a regular expression for those string which are ending(`$` --> refers to ending of the string in regex) with ing,s,e,able and string length must be minimum of 4 characters.\n",
    "\n",
    "Eating --> In this we observe that `ing` at ending of the string. So our reg_stemmer is removing that ing and giving 'eat'.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_9_'></a>[`LancasterStemmer`](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>The Lancaster Stemmer, also known as the Paice/Husk stemmer, is a rule-based stemming algorithm used in natural language processing to reduce words to their root form.</u> It applies a series of transformational rules iteratively to strip suffixes from words, aiming to condense related words to a common base or stem. The Lancaster Stemmer is known for being more aggressive and faster than other stemming algorithms like the Porter Stemmer, but this can sometimes lead to over-stemming where distinct words are reduced to the same root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eat\n",
      "writing ---> writ\n",
      "writes ---> writ\n",
      "programming ---> program\n",
      "programmer ---> program\n",
      "history ---> hist\n",
      "finally ---> fin\n",
      "finalized ---> fin\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "for i in words:\n",
    "    print(i+\" ---> \" +stemmer.stem(i))\n",
    "\n",
    "# .stem is a function to get stemming words from words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_10_'></a>[`Conclusion`](#toc0_)\n",
    "\n",
    "| Stemming Technique     | Importing Statement                    | Method        | Example                                                  |\n",
    "|------------------------|----------------------------------------|---------------|----------------------------------------------------------|\n",
    "| PorterStemmer          | `from nltk.stem import PorterStemmer`  | `PorterStemmer().stem()` | `stemmer = PorterStemmer()`<br>`word = \"running\"`<br>`stemmed_word = stemmer.stem(word)` |\n",
    "| SnowballStemmer        | `from nltk.stem import SnowballStemmer` | `SnowballStemmer().stem()` | `stemmer = SnowballStemmer(\"english\")`<br>`word = \"running\"`<br>`stemmed_word = stemmer.stem(word)` |\n",
    "| LancasterStemmer       | `from nltk.stem import LancasterStemmer` | `LancasterStemmer().stem()` | `stemmer = LancasterStemmer()`<br>`word = \"running\"`<br>`stemmed_word = stemmer.stem(word)` |\n",
    "| RegexpStemmer          | `from nltk.stem import RegexpStemmer` | `RegexpStemmer().stem()` | `stemmer = RegexpStemmer()`<br>`word = \"running\"`<br>`stemmed_word = stemmer.stem(word)` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Advantages:`\n",
    "- Reduces word variants\n",
    "- improved search results\n",
    "- Enhances text analysis\n",
    "\n",
    "`Disadvantages`\n",
    "- Loss of meaning\n",
    "- Over-stemming/Under stemming\n",
    "\n",
    "\n",
    "`Over-stemming` occurs when different words are reduced to the same stem even when they have different meanings.\n",
    "\n",
    "- In this case, if we apply a stemming algorithm like the Porter Stemmer or the Snowball Stemmer, all three words (\"running,\" \"runner,\" and \"runs\") will be stemmed to the same root \"run.\"\n",
    "\n",
    "- Stemming these words to the same root can lead to a loss of distinction between them, which may be undesirable in certain contexts.\n",
    "\n",
    "`Under-stemming` occurs when words with the same meaning are not reduced to the same stem.\n",
    "\n",
    "\n",
    "-  if we have the words \"cats\" and \"cat, Both \"cats\" and \"cat\" have the same meaning (plural and singular form of the noun \"cat\"), but the stemming algorithm fails to reduce them to the same stem, resulting in under-stemming.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_11_'></a>[`Lemmatization`](#toc0_)\n",
    "\n",
    "---\n",
    "\n",
    "`Defination`: Lemmatization technique is like stemming. The output we will get after lemmaitazation is called `lemma`. Which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
    "\n",
    "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "- NLTK provides wordNetLemmatizer class which is a thin wrapper around the corpus. This class uses morphy() function to the `wordNet CorpusReader` class to find a lemma.\n",
    "\n",
    "- Systematic process for reducing a token to its lemma( reduced form of word called as lemma).\n",
    "\n",
    "- Makes use of vocab, word sturcture, parts of speech tags and grammer relations before removing inflectional forms.\n",
    "\n",
    "Example:<br>\n",
    "- am, are, is >> be\n",
    "- running, ran, run, rans >> run\n",
    "\n",
    "Applications:\n",
    "\n",
    "- Chatbots\n",
    "- Text summerization\n",
    "- Q/A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'underestimate'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmetizer = WordNetLemmatizer()\n",
    "lemmetizer.lemmatize(\"underestimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eating\n",
      "eats ---> eats\n",
      "eaten ---> eaten\n",
      "writing ---> writing\n",
      "writes ---> writes\n",
      "programming ---> programming\n",
      "programmer ---> programmer\n",
      "history ---> history\n",
      "finally ---> finally\n",
      "finalized ---> finalized\n"
     ]
    }
   ],
   "source": [
    "for i in words:  \n",
    "    print(i+\" ---> \" +lemmetizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly ---> fairly\n",
      "sportingly ---> sportingly\n",
      "negative ---> negative\n",
      "goes ---> go\n",
      "congratulations ---> congratulation\n"
     ]
    }
   ],
   "source": [
    "for i in miss_words:  \n",
    "    print(i+\" ---> \" +lemmetizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\uppada\n",
      "[nltk_data]     satwik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading owm-1.4: Package 'owm-1.4' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('owm-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a id='toc3_12_'></a>[`Conclusion`](#toc0_)\n",
    "\n",
    "\n",
    "\n",
    "| Lemmatization Technique | Importing Statement                    | Method        | Example                                                  |\n",
    "|-------------------------|----------------------------------------|---------------|----------------------------------------------------------|\n",
    "| WordNetLemmatizer       | `from nltk.stem import WordNetLemmatizer` | `WordNetLemmatizer().lemmatize()` | `lemmatizer = WordNetLemmatizer()`<br>`word = \"running\"`<br>`lemmatized_word = lemmatizer.lemmatize(word)` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Stemming` vs `Lemmatization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\"\n",
    "# tokenise text\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Very', 'orderly', 'and', 'methodical', 'he', 'looked', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'ticking', 'a', 'sonorous', 'sermon', 'under', 'his', 'flapped', 'newly', 'bought', 'waist-coat', ',', 'a', 'though', 'it', 'pitted', 'it', 'gravity', 'and', 'longevity', 'against', 'the', 'levity', 'and', 'evanescence', 'of', 'the', 'brisk', 'fire', '.']\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['veri', 'orderli', 'and', 'method', 'he', 'look', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'tick', 'a', 'sonor', 'sermon', 'under', 'hi', 'flap', 'newli', 'bought', 'waist-coat', ',', 'as', 'though', 'it', 'pit', 'it', 'graviti', 'and', 'longev', 'against', 'the', 'leviti', 'and', 'evanesc', 'of', 'the', 'brisk', 'fire', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very</td>\n",
       "      <td>veri</td>\n",
       "      <td>Very</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orderly</td>\n",
       "      <td>orderli</td>\n",
       "      <td>orderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>methodical</td>\n",
       "      <td>method</td>\n",
       "      <td>methodical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>looked</td>\n",
       "      <td>look</td>\n",
       "      <td>looked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ticking</td>\n",
       "      <td>tick</td>\n",
       "      <td>ticking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sonorous</td>\n",
       "      <td>sonor</td>\n",
       "      <td>sonorous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>his</td>\n",
       "      <td>hi</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>flapped</td>\n",
       "      <td>flap</td>\n",
       "      <td>flapped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>newly</td>\n",
       "      <td>newli</td>\n",
       "      <td>newly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>pitted</td>\n",
       "      <td>pit</td>\n",
       "      <td>pitted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>its</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gravity</td>\n",
       "      <td>graviti</td>\n",
       "      <td>gravity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>longevity</td>\n",
       "      <td>longev</td>\n",
       "      <td>longevity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>levity</td>\n",
       "      <td>leviti</td>\n",
       "      <td>levity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>evanescence</td>\n",
       "      <td>evanesc</td>\n",
       "      <td>evanescence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  stemmed   lemmatized\n",
       "0          Very     veri         Very\n",
       "1       orderly  orderli      orderly\n",
       "3    methodical   method   methodical\n",
       "5        looked     look       looked\n",
       "18      ticking     tick      ticking\n",
       "20     sonorous    sonor     sonorous\n",
       "23          his       hi          his\n",
       "24      flapped     flap      flapped\n",
       "25        newly    newli        newly\n",
       "29           as       as            a\n",
       "32       pitted      pit       pitted\n",
       "33          its       it           it\n",
       "34      gravity  graviti      gravity\n",
       "36    longevity   longev    longevity\n",
       "39       levity   leviti       levity\n",
       "41  evanescence  evanesc  evanescence"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data={'token': tokens, 'stemmed': stemmed, 'lemmatized': lemmatized})\n",
    "df = df[['token', 'stemmed', 'lemmatized']]\n",
    "df[(df.token != df.stemmed) | (df.token != df.lemmatized)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## <a id='toc3_13_'></a>[`Stopswords with NLTK`](#toc0_)\n",
    "\n",
    "---\n",
    "| Stopword Technique      | Importing Statement                            | Method        | Example                                               |\n",
    "|-------------------------|------------------------------------------------|---------------|-------------------------------------------------------|\n",
    "| English Stopwords       | `from nltk.corpus import stopwords`            | `stopwords.words('english')` | `stop_words = stopwords.words('english')` |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_13_1_'></a>[Stop Words](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words in english: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') # we need to specify language\n",
    "print(\"Stop words in english: \",end='')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the stopwords which don't have a very big role while analysing the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph =\"\"\"My dear young friends,\n",
    "\n",
    "Today, as I stand before you, I am reminded of the immense potential that resides within each one of you. We are living in a world brimming with opportunities, waiting to be seized by those bold enough to dream and determined enough to pursue those dreams.\n",
    "\n",
    "Education is the key that unlocks the doors to these opportunities. It is the foundation upon which we build our future. I urge each one of you to embrace the power of education, for it is through knowledge that we empower ourselves and uplift our communities.\n",
    "\n",
    "But education alone is not enough. We must also foster a spirit of innovation and creativity. We live in an age of rapid technological advancement, where the only constant is change. In such times, those who dare to think differently, to question the status quo, are the ones who shape the course of history.\n",
    "\n",
    "Never underestimate the power of your dreams. Dream big, for dreams have the ability to transcend boundaries and defy limitations. But remember, dreams without action are merely fantasies. It is your actions that will turn your dreams into reality.\n",
    "\n",
    "As you embark on your journey, let not fear hold you back. Failure is but a stepping stone on the path to success. Embrace it, learn from it, and let it fuel your determination to persevere.\n",
    "\n",
    "Together, let us strive to create a world where every child has access to quality education, where innovation flourishes, and where dreams know no bounds.\n",
    "\n",
    "Thank you.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_13_2_'></a>[Using porterStemmer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My dear young friends,\\n\\nToday, as I stand before you, I am reminded of the immense potential that resides within each one of you.',\n",
       " 'We are living in a world brimming with opportunities, waiting to be seized by those bold enough to dream and determined enough to pursue those dreams.',\n",
       " 'Education is the key that unlocks the doors to these opportunities.',\n",
       " 'It is the foundation upon which we build our future.',\n",
       " 'I urge each one of you to embrace the power of education, for it is through knowledge that we empower ourselves and uplift our communities.',\n",
       " 'But education alone is not enough.',\n",
       " 'We must also foster a spirit of innovation and creativity.',\n",
       " 'We live in an age of rapid technological advancement, where the only constant is change.',\n",
       " 'In such times, those who dare to think differently, to question the status quo, are the ones who shape the course of history.',\n",
       " 'Never underestimate the power of your dreams.',\n",
       " 'Dream big, for dreams have the ability to transcend boundaries and defy limitations.',\n",
       " 'But remember, dreams without action are merely fantasies.',\n",
       " 'It is your actions that will turn your dreams into reality.',\n",
       " 'As you embark on your journey, let not fear hold you back.',\n",
       " 'Failure is but a stepping stone on the path to success.',\n",
       " 'Embrace it, learn from it, and let it fuel your determination to persevere.',\n",
       " 'Together, let us strive to create a world where every child has access to quality education, where innovation flourishes, and where dreams know no bounds.',\n",
       " 'Thank you.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Apply stopwords adn Filter then apply stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words) # converting all list of words into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my dear young friend , today , i stand , i remind immens potenti resid within one .',\n",
       " 'we live world brim opportun , wait seiz bold enough dream determin enough pursu dream .',\n",
       " 'educ key unlock door opportun .',\n",
       " 'it foundat upon build futur .',\n",
       " 'i urg one embrac power educ , knowledg empow uplift commun .',\n",
       " 'but educ alon enough .',\n",
       " 'we must also foster spirit innov creativ .',\n",
       " 'we live age rapid technolog advanc , constant chang .',\n",
       " 'in time , dare think differ , question statu quo , one shape cours histori .',\n",
       " 'never underestim power dream .',\n",
       " 'dream big , dream abil transcend boundari defi limit .',\n",
       " 'but rememb , dream without action mere fantasi .',\n",
       " 'it action turn dream realiti .',\n",
       " 'as embark journey , let fear hold back .',\n",
       " 'failur step stone path success .',\n",
       " 'embrac , learn , let fuel determin persever .',\n",
       " 'togeth , let us strive creat world everi child access qualiti educ , innov flourish , dream know bound .',\n",
       " 'thank .']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are seeing some Stopwords('I','It'..) . porterStemmer is not perform task upto mark. Let's try Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_13_3_'></a>[Using Snowball Stemmer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "## Apply stopwords adn Filter then apply stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snowball_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words) # converting all the words into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my dear young friend , today , i stand , i remind immens potenti resid within one .',\n",
       " 'we live world brim opportun , wait seiz bold enough dream determin enough pursu dream .',\n",
       " 'educ key unlock door opportun .',\n",
       " 'it foundat upon build futur .',\n",
       " 'i urg one embrac power educ , knowledg empow uplift communiti .',\n",
       " 'but educ alon enough .',\n",
       " 'we must also foster spirit innov creativ .',\n",
       " 'we live age rapid technolog advanc , constant chang .',\n",
       " 'in time , dare think differ , question status quo , one shape cours histori .',\n",
       " 'never underestim power dream .',\n",
       " 'dream big , dream abil transcend boundari defi limit .',\n",
       " 'but rememb , dream without action mere fantasi .',\n",
       " 'it action turn dream realiti .',\n",
       " 'as embark journey , let fear hold back .',\n",
       " 'failur step stone path success .',\n",
       " 'embrac , learn , let fuel determin persever .',\n",
       " 'togeth , let us strive creat world everi child access qualiti educ , innov flourish , dream know bound .',\n",
       " 'thank .']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that snowball makes every character into small characters and clean all the stop words betterthan porter stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_13_4_'></a>[Using lemmentization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Apply stopwords and Filter then apply stemming\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmetizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmetizer.lemmatize(word,pos='v') for word in words if word.lower() not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words) # converting all the words into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear young friends , Today , stand , remind immense potential reside within one .',\n",
       " 'live world brim opportunities , wait seize bold enough dream determine enough pursue dream .',\n",
       " 'Education key unlock doors opportunities .',\n",
       " 'foundation upon build future .',\n",
       " 'urge one embrace power education , knowledge empower uplift communities .',\n",
       " 'education alone enough .',\n",
       " 'must also foster spirit innovation creativity .',\n",
       " 'live age rapid technological advancement , constant change .',\n",
       " 'time , dare think differently , question status quo , ones shape course history .',\n",
       " 'Never underestimate power dream .',\n",
       " 'Dream big , dream ability transcend boundaries defy limitations .',\n",
       " 'remember , dream without action merely fantasy .',\n",
       " 'action turn dream reality .',\n",
       " 'embark journey , let fear hold back .',\n",
       " 'Failure step stone path success .',\n",
       " 'Embrace , learn , let fuel determination persevere .',\n",
       " 'Together , let us strive create world every child access quality education , innovation flourish , dream know bound .',\n",
       " 'Thank .']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every stop word is cleaned accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " `Corpus` --> `Sentences` --> `Words` --> `Check Stopwords` --> `Remove stopwords` --> `Stemming/Lemmatize Filtered words` --> `Proceed for further Preprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_14_'></a>[`Parts of Speech Tags`](#toc0_)\n",
    "\n",
    "\n",
    "--- \n",
    "### <a id='toc3_14_1_'></a>[POS Tagset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| POS Tag | Meaning                       | POS Tag | Meaning                       | POS Tag | Meaning                       |\n",
    "|---------|-------------------------------|---------|-------------------------------|---------|-------------------------------|\n",
    "| CC      | Coordinating conjunction     | CD      | Cardinal number               | DT      | Determiner                    |\n",
    "| EX      | Existential there             | FW      | Foreign word                  | IN      | Preposition or subordinating conjunction |\n",
    "| JJ      | Adjective                     | JJR     | Adjective, comparative        | JJS     | Adjective, superlative        |\n",
    "| LS      | List item marker             | MD      | Modal                         | NN      | Noun, singular or mass        |\n",
    "| NNS     | Noun, plural                 | NNP     | Proper noun, singular         | NNPS    | Proper noun, plural           |\n",
    "| PDT     | Predeterminer                | POS     | Possessive ending             | PRP     | Personal pronoun              |\n",
    "| PRP$    | Possessive pronoun           | RB      | Adverb                        | RBR     | Adverb, comparative           |\n",
    "| RBS     | Adverb, superlative          | RP      | Particle                      | SYM     | Symbol                        |\n",
    "| TO      | to                            | UH      | Interjection                  | VB      | Verb, base form               |\n",
    "| VBD     | Verb, past tense             | VBG     | Verb, gerund or present participle | VBN     | Verb, past participle      |\n",
    "| VBP     | Verb, non-3rd person singular present | VBZ     | Verb, 3rd person singular present | WDT     | Wh-determiner                 |\n",
    "| WP      | Wh-pronoun                   | WP$     | Possessive wh-pronoun         | WRB     | Wh-adverb                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph =\"\"\"My dear young friends,\n",
    "\n",
    "Today, as I stand before you, I am reminded of the immense potential that resides within each one of you. We are living in a world brimming with opportunities, waiting to be seized by those bold enough to dream and determined enough to pursue those dreams.\n",
    "\n",
    "Education is the key that unlocks the doors to these opportunities. It is the foundation upon which we build our future. I urge each one of you to embrace the power of education, for it is through knowledge that we empower ourselves and uplift our communities.\n",
    "\n",
    "But education alone is not enough. We must also foster a spirit of innovation and creativity. We live in an age of rapid technological advancement, where the only constant is change. In such times, those who dare to think differently, to question the status quo, are the ones who shape the course of history.\n",
    "\n",
    "Never underestimate the power of your dreams. Dream big, for dreams have the ability to transcend boundaries and defy limitations. But remember, dreams without action are merely fantasies. It is your actions that will turn your dreams into reality.\n",
    "\n",
    "As you embark on your journey, let not fear hold you back. Failure is but a stepping stone on the path to success. Embrace it, learn from it, and let it fuel your determination to persevere.\n",
    "\n",
    "Together, let us strive to create a world where every child has access to quality education, where innovation flourishes, and where dreams know no bounds.\n",
    "\n",
    "Thank you.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_14_2_'></a>[POS Tag](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dear', 'JJ'), ('young', 'JJ'), ('friends', 'NNS'), (',', ','), ('Today', 'NNP'), (',', ','), ('stand', 'NN'), (',', ','), ('reminded', 'VBD'), ('immense', 'JJ'), ('potential', 'JJ'), ('resides', 'NNS'), ('within', 'IN'), ('one', 'CD'), ('.', '.')]\n",
      "[('living', 'VBG'), ('world', 'NN'), ('brimming', 'VBG'), ('opportunities', 'NNS'), (',', ','), ('waiting', 'VBG'), ('seized', 'VBN'), ('bold', 'JJ'), ('enough', 'JJ'), ('dream', 'NN'), ('determined', 'VBD'), ('enough', 'JJ'), ('pursue', 'NN'), ('dreams', 'NNS'), ('.', '.')]\n",
      "[('Education', 'NN'), ('key', 'NN'), ('unlocks', 'JJ'), ('doors', 'NNS'), ('opportunities', 'NNS'), ('.', '.')]\n",
      "[('foundation', 'NN'), ('upon', 'IN'), ('build', 'JJ'), ('future', 'NN'), ('.', '.')]\n",
      "[('urge', 'NN'), ('one', 'CD'), ('embrace', 'NN'), ('power', 'NN'), ('education', 'NN'), (',', ','), ('knowledge', 'VB'), ('empower', 'JJR'), ('uplift', 'JJ'), ('communities', 'NNS'), ('.', '.')]\n",
      "[('education', 'NN'), ('alone', 'RB'), ('enough', 'RB'), ('.', '.')]\n",
      "[('must', 'MD'), ('also', 'RB'), ('foster', 'VB'), ('spirit', 'JJ'), ('innovation', 'NN'), ('creativity', 'NN'), ('.', '.')]\n",
      "[('live', 'JJ'), ('age', 'NN'), ('rapid', 'JJ'), ('technological', 'JJ'), ('advancement', 'NN'), (',', ','), ('constant', 'JJ'), ('change', 'NN'), ('.', '.')]\n",
      "[('times', 'NNS'), (',', ','), ('dare', 'NN'), ('think', 'VBP'), ('differently', 'RB'), (',', ','), ('question', 'NN'), ('status', 'NN'), ('quo', 'NN'), (',', ','), ('ones', 'NNS'), ('shape', 'VBP'), ('course', 'NN'), ('history', 'NN'), ('.', '.')]\n",
      "[('Never', 'RB'), ('underestimate', 'JJ'), ('power', 'NN'), ('dreams', 'NNS'), ('.', '.')]\n",
      "[('Dream', 'NNP'), ('big', 'JJ'), (',', ','), ('dreams', 'JJ'), ('ability', 'NN'), ('transcend', 'NN'), ('boundaries', 'NNS'), ('defy', 'VBP'), ('limitations', 'NNS'), ('.', '.')]\n",
      "[('remember', 'VB'), (',', ','), ('dreams', 'NNS'), ('without', 'IN'), ('action', 'NN'), ('merely', 'RB'), ('fantasies', 'NNS'), ('.', '.')]\n",
      "[('actions', 'NNS'), ('turn', 'VBP'), ('dreams', 'JJ'), ('reality', 'NN'), ('.', '.')]\n",
      "[('embark', 'NN'), ('journey', 'NN'), (',', ','), ('let', 'VB'), ('fear', 'NN'), ('hold', 'VB'), ('back', 'RB'), ('.', '.')]\n",
      "[('Failure', 'NN'), ('stepping', 'VBG'), ('stone', 'JJ'), ('path', 'NN'), ('success', 'NN'), ('.', '.')]\n",
      "[('Embrace', 'NN'), (',', ','), ('learn', 'NN'), (',', ','), ('let', 'VB'), ('fuel', 'JJ'), ('determination', 'NN'), ('persevere', 'RB'), ('.', '.')]\n",
      "[('Together', 'RB'), (',', ','), ('let', 'VB'), ('us', 'PRP'), ('strive', 'VB'), ('create', 'JJ'), ('world', 'NN'), ('every', 'DT'), ('child', 'NN'), ('access', 'NN'), ('quality', 'NN'), ('education', 'NN'), (',', ','), ('innovation', 'NN'), ('flourishes', 'NNS'), (',', ','), ('dreams', 'NNS'), ('know', 'VBP'), ('bounds', 'NNS'), ('.', '.')]\n",
      "[('Thank', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize   # importing sent tokenize function \n",
    "from nltk import word_tokenize  # import word tokenize function\n",
    "from nltk.corpus import stopwords # importing stops words\n",
    "import nltk                       # importing nltk\n",
    "\n",
    "sentences = sent_tokenize(paragraph)  # Paragraph --> sentense tokenization\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])  # sentences to word tokenization\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]  # Removing Stopwords and storing filtered words to another variable\n",
    "    print(nltk.pos_tag(filtered_words))  # applying pos tag for filtered words only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pos tag always need a parameter as list of words, Why i mention this...let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokens: expected a list of strings, got a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMy name is Satwik\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\uppada satwik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tag\\__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\uppada satwik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tag\\__init__.py:120\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Throws Error if tokens is of string type\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: tokens: expected a list of strings, got a string"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(\"My name is Satwik\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because pos tag can only perform tagging on a single list of tokens not with single words or entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Satwik.', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(\"My name is Satwik.\".split())) \n",
    "# It will make the list of words first then apply pos tag to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_14_3_'></a>[POS Tag sent](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokens: expected a list of strings, got a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy name is Satwik\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag_sents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\uppada satwik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tag\\__init__.py:184\u001b[0m, in \u001b[0;36mpos_tag_sents\u001b[1;34m(sentences, tagset, lang)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to tag the\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mgiven list of sentences, each consisting of a list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m:rtype: list(list(tuple(str, str)))\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    183\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences]\n",
      "File \u001b[1;32mc:\\Users\\uppada satwik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tag\\__init__.py:120\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Throws Error if tokens is of string type\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: tokens: expected a list of strings, got a string"
     ]
    }
   ],
   "source": [
    "words = \"My name is Satwik\".split()\n",
    "print(nltk.pos_tag_sents(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **POS_TAG_SENTS require list of sentences or strings not a string at time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Satwik.and', 'NNP'), ('i', 'NN'), (\"'m\", 'VBP'), ('from', 'IN'), ('Andhra', 'NNP'), ('pradesh', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Input string\n",
    "sentence = \"My name is Satwik.and i'm from Andhra pradesh.\"\n",
    "\n",
    "# Split the input string into sentences\n",
    "sentences = nltk.sent_tokenize(sentence)\n",
    "\n",
    "# Tokenize each sentence\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Perform POS tagging on the tokenized sentences\n",
    "pos_tags = nltk.pos_tag_sents(tokenized_sentences)\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_tag output: [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Satwik', 'NNP'), ('.', '.')]\n",
      "pos_tag_sents output: [[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Satwik', 'NNP'), ('.', '.')], [('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('data', 'NN'), ('scientist', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Example using pos_tag\n",
    "tokens = nltk.word_tokenize(\"My name is Satwik.\")\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"pos_tag output:\", pos_tags)\n",
    "\n",
    "# Example using pos_tag_sents\n",
    "sentences = [\"My name is Satwik.\", \"I am a data scientist.\"]\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "pos_tags_sents = nltk.pos_tag_sents(tokenized_sentences)\n",
    "print(\"pos_tag_sents output:\", pos_tags_sents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature             | pos_tag                        | pos_tag_sents                           |\n",
    "|---------------------|--------------------------------|-----------------------------------------|\n",
    "| Input Format        | List of tokens (words)         | List of sentences, each as a list of tokens |\n",
    "| Output Format       | List of tuples (word, tag)     | List of lists of tuples (word, tag)     |\n",
    "| Usage               | Single sentence or list of tokens | Multiple sentences represented as lists of tokens |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_15_'></a>[`Names Entity Recognition`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, \n",
    "whose company specialized in building metal frameworks and structures.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the named entity recognition is all about finding the name,place,date,money,organization, percent ..etc from the given text\n",
    "\n",
    "---\n",
    "\n",
    "|  Name  | Place | Date | Money | Organization |\n",
    "| ------ | ----- | ---- | -------- | ------------ |\n",
    "|satwik|Jalandhar|2003|20 ruppes|Lpu|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Eiffel', 'NNP'),\n",
       " ('Tower', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('built', 'VBN'),\n",
       " ('from', 'IN'),\n",
       " ('1887', 'CD'),\n",
       " ('to', 'TO'),\n",
       " ('1889', 'CD'),\n",
       " ('by', 'IN'),\n",
       " ('French', 'JJ'),\n",
       " ('engineer', 'NN'),\n",
       " ('Gustave', 'NNP'),\n",
       " ('Eiffel', 'NNP'),\n",
       " (',', ','),\n",
       " ('whose', 'WP$'),\n",
       " ('company', 'NN'),\n",
       " ('specialized', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('building', 'NN'),\n",
       " ('metal', 'NN'),\n",
       " ('frameworks', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('structures', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tagged_ele = nltk.pos_tag(words)\n",
    "Tagged_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1424.0,168.0\" width=\"1424px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"2.80899%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.40449%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.42697%\" x=\"2.80899%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Tower</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.02247%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.80899%\" x=\"11.236%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.6404%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.93258%\" x=\"14.0449%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">built</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.0112%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.37079%\" x=\"17.9775%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.6629%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.37079%\" x=\"21.3483%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1887</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.0337%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.24719%\" x=\"24.7191%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.8427%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.37079%\" x=\"26.9663%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1889</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.6517%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.24719%\" x=\"30.3371%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.4607%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.49438%\" x=\"32.5843%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">French</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.8315%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.61798%\" x=\"37.0787%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">engineer</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.8876%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.55056%\" x=\"42.6966%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Gustave</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.4719%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.68539%\" x=\"52.2472%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0899%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.93258%\" x=\"53.9326%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">whose</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">WP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.8989%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.05618%\" x=\"57.8652%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">company</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.3933%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.30337%\" x=\"62.9213%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">specialized</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.573%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.24719%\" x=\"70.2247%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.3483%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.61798%\" x=\"72.4719%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">building</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75.2809%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.93258%\" x=\"78.0899%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">metal</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.0562%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.74157%\" x=\"82.0225%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">frameworks</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.3933%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.80899%\" x=\"88.764%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.1685%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.74157%\" x=\"91.573%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">structures</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.9438%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.68539%\" x=\"98.3146%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.1573%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Eiffel', 'NNP'), ('Tower', 'NNP')]), ('was', 'VBD'), ('built', 'VBN'), ('from', 'IN'), ('1887', 'CD'), ('to', 'TO'), ('1889', 'CD'), ('by', 'IN'), Tree('GPE', [('French', 'JJ')]), ('engineer', 'NN'), Tree('PERSON', [('Gustave', 'NNP'), ('Eiffel', 'NNP')]), (',', ','), ('whose', 'WP$'), ('company', 'NN'), ('specialized', 'VBD'), ('in', 'IN'), ('building', 'NN'), ('metal', 'NN'), ('frameworks', 'NNS'), ('and', 'CC'), ('structures', 'NNS'), ('.', '.')])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.ne_chunk(Tagged_ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Bag of Words`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create Vocabulary: List all unique words from the text data.\n",
    "\n",
    "2) Vector Representation: Each text is represented as a vector, where each position in the vector corresponds to a word in the vocabulary.\n",
    "\n",
    "3) Word Count: The value in each position is the count of the word in the text.\n",
    "\n",
    "The Bag-of-Words model is often used as a starting point in text analysis before applying more sophisticated techniques like TF-IDF, Word2Vec, or neural network-based embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gangs of Wasseypur is a great movie.', 'The success of a movie depends on the performance of the actors.', 'There are no new movies releasing this week.']\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Gangs of Wasseypur is a great movie.\", \"The success of a movie depends on the performance of the actors.\", \"There are no new movies releasing this week.\"]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gangs wasseypur great movie .', 'success movie depends performance actors .', 'new movies releasing week .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\uppada\n",
      "[nltk_data]     satwik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\uppada\n",
      "[nltk_data]     satwik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess(document):\n",
    "    'changes document to lower case and removes stopwords'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document\n",
    "\n",
    "documents = [preprocess(document) for document in documents]\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 0 0 0 0 1 0]\n",
      " [1 1 0 0 1 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_model = vectorizer.fit_transform(documents)\n",
    "print(bow_model.toarray())  # returns the rown and column number of cells which have 1 as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12)\n",
      "['actors' 'depends' 'gangs' 'great' 'movie' 'movies' 'new' 'performance'\n",
      " 'releasing' 'success' 'wasseypur' 'week']\n"
     ]
    }
   ],
   "source": [
    "print(bow_model.shape)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actors</th>\n",
       "      <th>depends</th>\n",
       "      <th>gangs</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>movies</th>\n",
       "      <th>new</th>\n",
       "      <th>performance</th>\n",
       "      <th>releasing</th>\n",
       "      <th>success</th>\n",
       "      <th>wasseypur</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actors  depends  gangs  great  movie  movies  new  performance  releasing  \\\n",
       "0       0        0      1      1      1       0    0            0          0   \n",
       "1       1        1      0      0      1       0    0            1          0   \n",
       "2       0        0      0      0      0       1    1            0          1   \n",
       "\n",
       "   success  wasseypur  week  \n",
       "0        0          1     0  \n",
       "1        1          0     0  \n",
       "2        0          0     1  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"Bag of words\")\n",
    "pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`process`**\n",
    "\n",
    "word tokenization --> remove stopwords --> initilize countvectorizer --> transform text to numbers --> modify it to array and give colummn name as get_feature_names_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `CANONICALISATION`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `phonetic hashing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `EDIT DISTANCE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levenshtein Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "edit_distance('apple','appel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Damerau–Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('apple','appel', transpositions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_16_'></a>[`Text Encoding`](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically Text encoding is implemented in various ways:\n",
    "- 1. One-hot encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_17_'></a>[`One - Hot Encoding`](#toc0_)\n",
    "\n",
    "---\n",
    "One- hot encoding represents each word in the vocabulary as a binary vector where only one bit is set to 1 (hot) and all others are set to 0(cold)\n",
    "\n",
    "Each word in the vocabulary is assigned a unique index, and the binary vector has the length equal to the size of the vocabulary.\n",
    "\n",
    "NLTK provides the `nltk.one_hot` function to perform one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
